{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data Quality Monitoring Framework \n",
        "**- # Covers: Null checks, Integer checks, and Date checks**\n",
        "\n",
        "**- # Classifies records into good/bad based on rule actions (Reject vs Flag)**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import DataFrame\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import (\n",
        "    StringType, IntegerType, LongType, ShortType, ByteType,\n",
        "    DoubleType, FloatType, DateType, TimestampType\n",
        ")\n",
        "import json\n",
        "import sys\n",
        "from typing import List, Dict, Any, Optional"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkpool1",
              "statement_id": 2,
              "statement_ids": [
                2
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "514",
              "normalized_state": "finished",
              "queued_time": "2025-11-19T17:48:18.1399931Z",
              "session_start_time": "2025-11-19T17:48:20.4300138Z",
              "execution_start_time": "2025-11-19T17:50:13.3646074Z",
              "execution_finish_time": "2025-11-19T17:50:13.6603807Z",
              "parent_msg_id": "ea7d61a3-4bf4-4ac3-a883-5739ae0fff11"
            },
            "text/plain": "StatementMeta(sparkpool1, 514, 2, Finished, Available, Finished)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 7,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "# Utility helpers\n",
        "# -------------------------------\n",
        "\n",
        "INTEGER_TYPES = (IntegerType, LongType, ShortType, ByteType)\n",
        "NUMERIC_TYPES = INTEGER_TYPES + (DoubleType, FloatType)\n",
        "\n",
        "def _col_exists(df: DataFrame, col: str) -> bool:\n",
        "    return col in df.columns\n",
        "\n",
        "def _dtype(df: DataFrame, col: str):\n",
        "    return df.schema[col].dataType if _col_exists(df, col) else None\n",
        "\n",
        "def _null_invalid_expr(df: DataFrame, col: str, treat_blank_as_null: bool = True):\n",
        "    \"\"\"\n",
        "    Returns a boolean Column that is True when the value is considered invalid due to null/blank/NaN.\n",
        "    \"\"\"\n",
        "    dt = _dtype(df, col)\n",
        "    if dt is None:\n",
        "        return F.lit(True)  # Missing column -> invalid\n",
        "    c = F.col(col)\n",
        "    if isinstance(dt, StringType):\n",
        "        blank = F.length(F.trim(c)) == 0\n",
        "        return c.isNull() | (blank if treat_blank_as_null else F.lit(False))\n",
        "    elif isinstance(dt, (DoubleType, FloatType)):\n",
        "        return c.isNull() | F.isnan(c)\n",
        "    else:\n",
        "        return c.isNull()\n",
        "\n",
        "def _integer_invalid_expr(\n",
        "    df: DataFrame,\n",
        "    col: str,\n",
        "    min_value: Optional[int] = None,\n",
        "    max_value: Optional[int] = None,\n",
        "    allow_string_integers: bool = True\n",
        "):\n",
        "    \"\"\"\n",
        "    Returns a boolean Column that is True when the value is invalid as an integer\n",
        "    (null/blank/non-integer/out of range).\n",
        "    \"\"\"\n",
        "    dt = _dtype(df, col)\n",
        "    if dt is None:\n",
        "        return F.lit(True)\n",
        "\n",
        "    c = F.col(col)\n",
        "\n",
        "    if isinstance(dt, INTEGER_TYPES):\n",
        "        base_invalid = c.isNull()\n",
        "        bounds_invalid = F.lit(False)\n",
        "        if min_value is not None:\n",
        "            bounds_invalid = bounds_invalid | (c < F.lit(min_value))\n",
        "        if max_value is not None:\n",
        "            bounds_invalid = bounds_invalid | (c > F.lit(max_value))\n",
        "        return base_invalid | bounds_invalid\n",
        "\n",
        "    if isinstance(dt, StringType) and allow_string_integers:\n",
        "        s = F.trim(c)\n",
        "        is_blank = F.length(s) == 0\n",
        "        is_int_str = s.rlike(r'^[+-]?\\d+$')\n",
        "        casted = s.cast(\"long\")\n",
        "        bounds_invalid = F.lit(False)\n",
        "        if min_value is not None:\n",
        "            bounds_invalid = bounds_invalid | (casted < F.lit(min_value))\n",
        "        if max_value is not None:\n",
        "            bounds_invalid = bounds_invalid | (casted > F.lit(max_value))\n",
        "        return is_blank | (~is_int_str) | bounds_invalid\n",
        "\n",
        "    # Float/Double or other types -> treat as invalid for integer check\n",
        "    return F.lit(True)\n",
        "\n",
        "def _date_invalid_expr(\n",
        "    df: DataFrame,\n",
        "    col: str,\n",
        "    fmt: str = \"yyyy-MM-dd\",\n",
        "    min_date: Optional[str] = None,  # string formatted according to fmt\n",
        "    max_date: Optional[str] = None   # string formatted according to fmt\n",
        "):\n",
        "    \"\"\"\n",
        "    Returns a boolean Column that is True when the value is invalid as a date:\n",
        "      - missing column\n",
        "      - null/blank\n",
        "      - not parseable with given format (for strings)\n",
        "      - outside [min_date, max_date] if provided\n",
        "    Notes:\n",
        "      - For Date/Timestamp columns, blank check is not needed; null is invalid.\n",
        "      - For strings, parsing failure yields null -> invalid.\n",
        "    \"\"\"\n",
        "    dt = _dtype(df, col)\n",
        "    if dt is None:\n",
        "        return F.lit(True)\n",
        "\n",
        "    c = F.col(col)\n",
        "\n",
        "    # Parse to date\n",
        "    if isinstance(dt, StringType):\n",
        "        s = F.trim(c)\n",
        "        is_blank = F.length(s) == 0\n",
        "        parsed = F.to_date(s, fmt)\n",
        "        parse_invalid = parsed.isNull()\n",
        "    elif isinstance(dt, TimestampType):\n",
        "        parsed = F.to_date(c)\n",
        "        parse_invalid = parsed.isNull()\n",
        "        is_blank = F.lit(False)\n",
        "    elif isinstance(dt, DateType):\n",
        "        parsed = c\n",
        "        parse_invalid = parsed.isNull()\n",
        "        is_blank = F.lit(False)\n",
        "    else:\n",
        "        # Unsupported types -> invalid\n",
        "        return F.lit(True)\n",
        "\n",
        "    bounds_invalid = F.lit(False)\n",
        "    if min_date is not None:\n",
        "        min_col = F.to_date(F.lit(min_date), fmt)\n",
        "        bounds_invalid = bounds_invalid | (parsed < min_col)\n",
        "    if max_date is not None:\n",
        "        max_col = F.to_date(F.lit(max_date), fmt)\n",
        "        bounds_invalid = bounds_invalid | (parsed > max_col)\n",
        "\n",
        "    return is_blank | parse_invalid | bounds_invalid\n",
        "\n",
        "# -------------------------------\n",
        "# Rule application\n",
        "# -------------------------------\n",
        "\n",
        "def _row_to_rule_dict(row) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Convert a rule table/dataset row to a normalized dict.\n",
        "    Expected columns in rule table/dataset:\n",
        "      - rule_id\n",
        "      - rule_description\n",
        "      - dataset_name\n",
        "      - field_name\n",
        "      - action   (e.g., Reject, Flag)\n",
        "      - error_description\n",
        "      - rule_type ('NULL_CHECK' | 'INTEGER_CHECK' | 'DATE_CHECK')\n",
        "      - params (JSON string), optional:\n",
        "          For NULL_CHECK:\n",
        "            {\"treat_blank_as_null\": true}\n",
        "          For INTEGER_CHECK:\n",
        "            {\"min\": 0, \"max\": 100, \"allow_string_integers\": true}\n",
        "          For DATE_CHECK:\n",
        "            {\"format\": \"yyyy-MM-dd\", \"min_date\": \"2020-01-01\", \"max_date\": \"2030-12-31\"}\n",
        "    \"\"\"\n",
        "    d = row.asDict(recursive=True)\n",
        "    params = d.get(\"params\")\n",
        "    try:\n",
        "        d[\"params\"] = json.loads(params) if isinstance(params, str) and params else {}\n",
        "    except Exception:\n",
        "        d[\"params\"] = {}\n",
        "    return d\n",
        "\n",
        "def apply_dq_rules(\n",
        "    df: DataFrame,\n",
        "    rules_df: DataFrame,\n",
        "    dataset_name: Optional[str] = None\n",
        "):\n",
        "    \"\"\"\n",
        "    Apply data quality rules to a DataFrame and classify records.\n",
        "\n",
        "    Returns:\n",
        "      result_df: df + violations columns and classification flags\n",
        "      good_df: records without Reject violations\n",
        "      bad_df:  records with at least one Reject violation\n",
        "      summary_df: counts by violation message\n",
        "    \"\"\"\n",
        "    # Filter and collect rules to driver\n",
        "    filtered = rules_df\n",
        "    if dataset_name:\n",
        "        filtered = filtered.filter(F.col(\"dataset_name\") == F.lit(dataset_name))\n",
        "    # if table_name:\n",
        "    #     filtered = filtered.filter(F.col(\"table_name\") == F.lit(table_name))\n",
        "\n",
        "    rules = [ _row_to_rule_dict(r) for r in filtered.collect() ]\n",
        "\n",
        "    # Build violation expressions\n",
        "    viol_all_exprs: List[Any] = []\n",
        "    viol_reject_exprs: List[Any] = []\n",
        "    viol_flag_exprs: List[Any] = []\n",
        "\n",
        "    for r in rules:\n",
        "        rid = r.get(\"rule_id\")\n",
        "        rtype = r.get(\"rule_type\")\n",
        "        field = r.get(\"field_name\")\n",
        "        severity = r.get(\"severity\", \"Warning\")\n",
        "        action = r.get(\"action\", \"Flag\")\n",
        "        err = r.get(\"error_description\", r.get(\"rule_description\", \"Rule violation\"))\n",
        "        p = r.get(\"params\", {}) or {}\n",
        "\n",
        "        # Choose invalid expression based on rule type\n",
        "        if rtype == \"NULL_CHECK\":\n",
        "            invalid = _null_invalid_expr(df, field, p.get(\"treat_blank_as_null\", True))\n",
        "        elif rtype == \"INTEGER_CHECK\":\n",
        "            invalid = _integer_invalid_expr(\n",
        "                df, field,\n",
        "                min_value=p.get(\"min\"),\n",
        "                max_value=p.get(\"max\"),\n",
        "                allow_string_integers=p.get(\"allow_string_integers\", True)\n",
        "            )\n",
        "        elif rtype == \"DATE_CHECK\":\n",
        "            invalid = _date_invalid_expr(\n",
        "                df, field,\n",
        "                fmt=p.get(\"format\", \"yyyy-MM-dd\"),\n",
        "                min_date=p.get(\"min_date\"),\n",
        "                max_date=p.get(\"max_date\")\n",
        "            )\n",
        "        else:\n",
        "            # Unknown rule -> consider invalid to draw attention\n",
        "            invalid = F.lit(True)\n",
        "\n",
        "        msg = f\"{rid} | {field} | {err} | severity={severity} | action={action}\"\n",
        "        msg_col = F.when(invalid, F.lit(msg)).otherwise(F.lit(None))\n",
        "\n",
        "        viol_all_exprs.append(msg_col)\n",
        "        if action.upper() == \"REJECT\":\n",
        "            viol_reject_exprs.append(msg_col)\n",
        "        else:\n",
        "            viol_flag_exprs.append(msg_col)\n",
        "\n",
        "    # Create arrays and remove nulls\n",
        "    result_df = df\n",
        "    if viol_all_exprs:\n",
        "        result_df = result_df.withColumn(\"violations_all\", F.array(*viol_all_exprs)) \\\n",
        "                             .withColumn(\"violations_all\", F.expr(\"filter(violations_all, x -> x is not null)\"))\n",
        "    else:\n",
        "        result_df = result_df.withColumn(\"violations_all\", F.array())\n",
        "\n",
        "    if viol_reject_exprs:\n",
        "        result_df = result_df.withColumn(\"violations_reject\", F.array(*viol_reject_exprs)) \\\n",
        "                             .withColumn(\"violations_reject\", F.expr(\"filter(violations_reject, x -> x is not null)\"))\n",
        "    else:\n",
        "        result_df = result_df.withColumn(\"violations_reject\", F.array())\n",
        "\n",
        "    if viol_flag_exprs:\n",
        "        result_df = result_df.withColumn(\"violations_flag\", F.array(*viol_flag_exprs)) \\\n",
        "                             .withColumn(\"violations_flag\", F.expr(\"filter(violations_flag, x -> x is not null)\"))\n",
        "    else:\n",
        "        result_df = result_df.withColumn(\"violations_flag\", F.array())\n",
        "\n",
        "    # Summaries and classification\n",
        "    result_df = result_df.withColumn(\"violation_summary\", F.expr(\"concat_ws('; ', violations_all)\")) \\\n",
        "                         .withColumn(\"is_bad\", F.size(F.col(\"violations_reject\")) > F.lit(0)) \\\n",
        "                         .withColumn(\"is_flagged\", F.size(F.col(\"violations_flag\")) > F.lit(0))\n",
        "\n",
        "    bad_df = result_df.filter(F.col(\"is_bad\"))\n",
        "    good_df = result_df.filter(~F.col(\"is_bad\"))\n",
        "\n",
        "    bad_df = bad_df.withColumn('violations_all', F.concat_ws('|', F.col('violations_all')))\\\n",
        "                .withColumn('violations_reject', F.concat_ws('|', F.col('violations_all')))\\\n",
        "                .withColumn('violations_flag', F.concat_ws('|', F.col('violations_all')))\n",
        "    \n",
        "    \n",
        "    good_df = good_df.withColumn('violations_all', F.concat_ws('|', F.col('violations_all')))\\\n",
        "                .withColumn('violations_reject', F.concat_ws('|', F.col('violations_all')))\\\n",
        "                .withColumn('violations_flag', F.concat_ws('|', F.col('violations_all')))\n",
        "\n",
        "    summary_df = result_df.select(F.explode(F.col(\"violations_all\")).alias(\"violation\")) \\\n",
        "                          .groupBy(\"violation\").agg(F.count(\"*\").alias(\"count\")) \\\n",
        "                          .orderBy(F.desc(\"count\"))\n",
        "\n",
        "    return result_df, good_df, bad_df, summary_df\n",
        "\n",
        "# -------------------------------\n",
        "# ADLS write helpers\n",
        "# -------------------------------\n",
        "\n",
        "def write_records_to_adls(bad_df: DataFrame, invalid_rec_adls_path: str,good_df: DataFrame, valid_rec_adls_path: str, mode: str = \"append\"):\n",
        "    \"\"\"\n",
        "    Writes bad records (including violation metadata) to ADLS.\n",
        "    adls_path example: \"abfss://dq@myaccount.dfs.core.windows.net/bad/my_dataset/my_table/\"\n",
        "    \"\"\"\n",
        "    try:\n",
        "        bad_df.write.mode(mode).csv(invalid_rec_adls_path)\n",
        "        print(f\"Wrote bad records to {invalid_rec_adls_path}\")\n",
        "        good_df.write.mode(mode).csv(valid_rec_adls_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to write to ADLS: {e}\", file=sys.stderr)\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkpool1",
              "statement_id": 37,
              "statement_ids": [
                37
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "514",
              "normalized_state": "finished",
              "queued_time": "2025-11-19T18:50:27.9680575Z",
              "session_start_time": null,
              "execution_start_time": "2025-11-19T18:50:27.9706515Z",
              "execution_finish_time": "2025-11-19T18:50:28.2353661Z",
              "parent_msg_id": "681e5858-d813-4895-9c9b-7b106fee59b1"
            },
            "text/plain": "StatementMeta(sparkpool1, 514, 37, Finished, Available, Finished)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 42,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "dqm_file_path = <provide your adls path here>\n",
        "rules_df = spark.read.option('header','true').option('inferSchema','true').csv(dqm_file_path)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkpool1",
              "statement_id": 19,
              "statement_ids": [
                19
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "514",
              "normalized_state": "finished",
              "queued_time": "2025-11-19T18:29:37.0116454Z",
              "session_start_time": null,
              "execution_start_time": "2025-11-19T18:29:37.0135156Z",
              "execution_finish_time": "2025-11-19T18:29:40.058938Z",
              "parent_msg_id": "c75e3f94-96a6-46d7-8dff-8dfb27062bb8"
            },
            "text/plain": "StatementMeta(sparkpool1, 514, 19, Finished, Available, Finished)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 24,
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}